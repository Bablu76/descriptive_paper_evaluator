{
  "answers": [
    {
      "question": "Define Space Complexity.  Explain the components used to compute space complexity.",
      "marks": 2,
      "CO": "co1",
      "keywords": [
        "Space complexity",
        "memory space",
        "algorithm",
        "execution",
        "input data",
        "auxiliary space",
        "stack",
        "components",
        "fixed part",
        "variable part",
        "input size",
        "dynamically allocated memory",
        "recursion stack",
        "return address",
        "memory footprint"
      ],
      "model_answer": "Space complexity refers to the total amount of memory space an algorithm utilizes during its execution. This includes the space occupied by the input data, auxiliary space used by the algorithm, and the space for the program's stack.\n\nThe space complexity is computed by considering two primary components. The fixed part is the space required to store instructions, constants, and fixed-size variables, independent of the input size. The variable part includes the space needed for variables whose size depends on the input, such as dynamically allocated memory or recursion stack space. Additionally, some space is needed for the return address. Therefore, space complexity provides a holistic view of an algorithm's memory footprint."
    },
    {
      "question": "Develop Find operation in Disjoint Sets.",
      "marks": 2,
      "CO": "co2",
      "keywords": [
        "Disjoint Sets",
        "Find operation",
        "representative element",
        "path compression",
        "optimization",
        "parent pointer",
        "root",
        "tree structure"
      ],
      "model_answer": "The Find operation in Disjoint Sets determines the representative element of the set to which a given element belongs. This operation is crucial for efficiently determining if two elements are in the same set.\n\n```\nfunction FIND(x)\n  if x.parent != x\n    x.parent = FIND(x.parent) // Path compression\n  return x.parent\n```\n\nThe pseudocode illustrates the Find operation with path compression. Path compression is a key optimization technique. During the Find operation, as the algorithm traverses the path to find the representative, it updates each node's parent pointer along the path to point directly to the root (representative). This flattens the tree structure, making subsequent Find operations on elements in that path faster. Effectively, it compresses the path, hence the name."
    },
    {
      "question": "Define Divide and Conquer strategy.",
      "marks": 2,
      "CO": "co3",
      "keywords": [
        "Divide and Conquer",
        "algorithmic paradigm",
        "subproblems",
        "divided",
        "conquered",
        "combined",
        "recursively",
        "Merge Sort"
      ],
      "model_answer": "Divide and Conquer is an algorithmic paradigm that solves a problem by recursively breaking it down into smaller, similar subproblems until these become simple enough to be solved directly.\n\nThe strategy consists of three key steps: First, the original problem is *divided* into multiple smaller, independent subproblems. Then, these subproblems are *conquered* by solving them recursively. Finally, the solutions to the subproblems are *combined* to produce the solution to the original problem. A classic example is Merge Sort, where the array is divided, sorted recursively, and then merged."
    },
    {
      "question": "Describe Performance Analysis. Illustrate with suitable examples.",
      "marks": 5,
      "CO": "co1",
      "keywords": [
        "Performance analysis",
        "algorithm",
        "efficiency",
        "resources",
        "computational time",
        "time complexity",
        "memory space",
        "space complexity",
        "scalability",
        "linear search",
        "binary search"
      ],
      "model_answer": "Performance analysis is the process of evaluating the efficiency of an algorithm, primarily focusing on the resources it consumes during execution. These resources typically encompass the computational time (time complexity) and the memory space (space complexity) required by the algorithm. The goal is to assess and compare the suitability of different algorithms for solving a particular problem, allowing developers to choose the most efficient option. Performance analysis is crucial as it directly impacts the responsiveness and scalability of software systems.\n\nTo illustrate, consider two algorithms designed to search for a specific element within a sorted array: linear search and binary search. Linear search sequentially checks each element of the array until the target element is found or the entire array has been traversed. In the worst-case scenario, where the target element is the last element or not present, linear search requires examining all 'n' elements, resulting in a time complexity of O(n).\n\nBinary search, on the other hand, exploits the sorted nature of the array. It repeatedly divides the search interval in half. If the middle element is the target, the search is successful. If the target is smaller, the search continues in the left half; otherwise, it continues in the right half. This halving process significantly reduces the search space. In the worst-case scenario, binary search requires approximately log\u2082(n) comparisons, resulting in a time complexity of O(log n).\n\nTherefore, for large arrays, binary search demonstrates significantly better performance than linear search. For example, when searching for an element in a sorted array of 1024 elements, linear search could potentially require 1024 comparisons, while binary search would require a maximum of 10 comparisons (since log\u2082(1024) = 10). This demonstrates the practical implications of performance analysis in choosing the optimal algorithm for a given task. This is not the only criteria; the more subtle approach will gradually infect your own program-writing habits so that you will automatically strive to achieve these goals."
    }
  ]
}