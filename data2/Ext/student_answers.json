{
  "students": [
    {
      "student_id": "student_answer",
      "answer": {
        "1.0a": ": The space complexity of an algorithm quantifies the memory consumption as a function of input size. Components used to compute space complexity include:\n\nFixed space: Space for instruction storage, simple variables, constants, and other space that doesn't depend on input size.\nVariable space: Space that depends on input size, including dynamically allocated memory, recursion stack space, and data structures whose size varies with input.\nReturn space: Space required for storing return values and parameters passed during function calls.",
        "1.0b": ": The Find operation in Disjoint Sets helps determine which set a particular element belongs to. Implementation with path compression:\nif parent[x] ≠ x:\nparent[x] = FIND(parent[x]) // Path compression\nreturn parent[x]\n\nPath compression flattens the tree structure, significantly reducing the height of the trees and improving the time complexity of subsequent operations.",
        "1.0c": ": Divide and Conquer is an algorithmic strategy that splits a problem into smaller subproblems, solves them recursively, and then merges their solutions.\nKey steps:\n\nDivide: Break the original problem into smaller subproblems.\nConquer: Recursively solve each subproblem.\nCombine: Merge the solutions of subproblems to solve the original problem.",
        "2.0b": ": Disjoint Sets' Union and Find operations with performance improvements:\nFind: Determines which set an element belongs to by identifying its root.\nUnion: Merges two sets by connecting their roots.\nPerformance improvements include:\na) Path Compression: During Find, update each visited node’s parent directly to the root, flattening the tree:\n\nini\nFINO-WITH-PATH-COMPRESSION(x):  \nif parent[x] ≠ x:  \n  parent[x] = FIND(parent[x]) // Path compression  \nreturn parent[x]  \nb) Union by Rank/Size: Attach the shallower tree under the deeper to keep trees shallow:\n\nini\nUNION-BY-RANK(x, y):  \nroot_x = FIND(x)  \nroot_y = FIND(y)  \nif root_x ≠ root_y:  \n  if rank[root_x] < rank[root_y]:  \n    parent[root_x] = root_y  \n  elseif rank[root_x] > rank[root_y]:  \n    parent[root_y] = root_x  \n  else:  \n    parent[root_y] = root_x  \n    rank[root_x] += 1  \nThe combination of these optimizations yields a highly efficient data structure with near-constant amortized time per operation, specifically approaching O(α(n)), with α(n) being the inverse Ackermann function, which grows very slowly and is practically constant for all realistic input sizes."
      }
    },
    {
      "student_id": "student_answer1",
      "answer": {
        "1.0a": "Space complexity of an algorithm is the total amount of memory it requires as a function of input size. It consists of:\n\nFixed space for instructions, constants and any variables whose size does not depend on n.\nVariable space for input-dependent data structures (e.g., arrays of size n) and the call/recursion stack.\nReturn space to hold the output before it is returned.\nFor example, MergeSort uses O(n) auxiliary array space (variable) atop its O(1) fixed working memory and O(1) return buffer.",
        "1.0b": "FIND(x):\nif parent[x] ≠ x:\nparent[x] = FIND(parent[x]) // path compression\nreturn parent[x]\n\nPath compression flattens the tree by making every node on the search path point directly to the root. In graph connectivity checks, this means subsequent FIND calls on those nodes take near-constant time.",
        "1.0c": "Divide-and-Conquer is a design paradigm that (1) divides the problem into smaller subproblems of the same form, (2) conquers each subproblem (typically via recursion), and (3) combines the sub-solutions into a final answer.\n\nFor example, QuickSort divides the array around a pivot, recursively sorts each part, then concatenates the results.",
        "2.0b": "Two operations:\n\nFind locates the set representative (root) of element x. With path compression, each visited node on the way to the root is re-linked directly to the root, flattening the tree.\nUnion merges two sets by linking one root under the other. With union by rank (or size), the shallower tree attaches under the deeper, keeping overall height small.\nThese optimizations drive the amortized cost per operation to O(α(n)), where α(n) is the inverse Ackermann function—a very slowly growing function. In practice, even for millions of elements, each UNION or FIND runs in effectively constant time. Algorithms like Kruskal's Minimum Spanning Tree use Union-Find with these enhancements to process edges in near-linear time. Similarly, social network clustering relies on them for quick connectivity queries."
      }
    }
  ]
}